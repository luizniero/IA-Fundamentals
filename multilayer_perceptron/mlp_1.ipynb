{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, architecture, activation, dropout_p=0.0):\n",
    "        super()__init__() # chama o construtor da classe pai.\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.activation_name = activation\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        for i in range(len(architecture) - 1):\n",
    "            self.layers.append(nn.Linear(architecture[i], architecture[i+1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            x = layer(x)\n",
    "            if self.activation_name == 'relu':\n",
    "                x = F.relu(x)\n",
    "            elif self.activation_name == 'tanh':\n",
    "                x = torch.tanh(x)\n",
    "            if self.dropout_p > 0:\n",
    "                x = F.dropout(x, p=self.dropout_p, training=self.training)\n",
    "        x = self.layers[-1](x)  # última camada (sem ativação)\n",
    "        return x\n",
    "\n",
    "\n",
    "# retorna um número entre 0 e 1 representando o quão bom foi. 0 = pior resultado (muito longe), 1 = acertou\n",
    "def binary_accuracy(preds, labels, threshold=0.5):\n",
    "    probs = torch.sigmoid(preds)\n",
    "    preds_class = (probs > threshold).float()\n",
    "    return (preds_class == labels).float().mean().item()\n",
    "\n",
    "# Retorna a proporção de exemplos que a rede previu corretamente.\n",
    "def multiclass_accuracy(preds, labels):\n",
    "    preds_class = preds.argmax(dim=1)                # escolhe classe com maior logit\n",
    "    return (preds_class == labels).float().mean().item()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_validation(model, optimizer, criterion, X_train, y_train, X_val, y_val, epochs, patience):\n",
    "    best_loss = float('inf')\n",
    "    best_model = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Treinamento\n",
    "        model.train() # Coloca a rede em modo treinamento. (ativa dropout, etc)\n",
    "        outputs = model(X_train) # faz o forward pass — aplica as camadas da rede nos dados de treino.\n",
    "        loss = criterion(outputs, y_train) # calcula a função de perda com as saídas da rede e os rótulos reais.\n",
    "        optimizer.zero_grad() # zera os gradientes antigos acumulados (importante!).\n",
    "        loss.backward() # calcula os gradientes da perda em relação aos pesos (backpropagation).\n",
    "        optimizer.step() # atualiza os pesos da rede com base nos gradientes.\n",
    "\n",
    "        # Avaliação\n",
    "        model.eval() # coloca a rede em modo avaliação (desativa dropout, etc).\n",
    "        with torch.no_grad(): # desativa o rastreamento de gradientes — melhora a performance e reduz uso de memória.\n",
    "            train_acc = accuracy(outputs, y_train) # Mede a acurácia no conjunto de treinamento\n",
    "            \n",
    "            if X_val and y_val:\n",
    "                val_outputs = model(X_val) # Obtém os outputs do conjunto de validação.\n",
    "                val_loss = criterion(val_outputs, y_val) # Calcula novamente a perda, agora com os dados de validação.\n",
    "                val_acc = accuracy(val_outputs, y_val) # Mede a acurácia no conjunto de validação.\n",
    "\n",
    "        # Armazena as métricas obtidas pelo treinamento/validação no dicionário history.\n",
    "        history['train_loss'].append(loss.item())\n",
    "        history['train_acc'].append(train_acc)\n",
    "\n",
    "        if X_val and y_val:\n",
    "            history['val_loss'].append(val_loss.item())\n",
    "            history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "        # Early stopping - with validation set\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # Restaura o melhor modelo salvo.\n",
    "    model.load_state_dict(best_model)\n",
    "    return model, history\n",
    "\n",
    "def train_model_no_validation(model, optimizer, criterion, X_train, y_train, epochs, patience):\n",
    "    best_loss = float('inf')\n",
    "    best_model = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    history = {'train_loss': [], 'train_acc': []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Treinamento\n",
    "        model.train()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Avaliação (em modo eval, mas nos mesmos dados de treino)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_acc = accuracy(outputs, y_train)\n",
    "\n",
    "        # Registro\n",
    "        history['train_loss'].append(loss.item())\n",
    "        history['train_acc'].append(train_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {loss.item():.4f}, Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "        # Early stopping baseado em perda de treino\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_model = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "    return model, history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_treinamento = '..\\\\perceptron_datasets\\\\train_dataset1.csv'\n",
    "dataset_testes = '..\\\\perceptron_datasets\\\\test_dataset1.csv'\n",
    "\n",
    "df_train_loaded = pd.read_csv(dataset_treinamento)\n",
    "df_test_loaded = pd.read_csv(dataset_testes)\n",
    "\n",
    "x_train = df_train_loaded.drop('label', axis=1).values\n",
    "y_train = df_train_loaded['label'].values\n",
    "\n",
    "x_test = df_test_loaded.drop('label', axis=1).values\n",
    "y_test = df_test_loaded['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimento 1\n",
    "Camadas = 2, 3, 1\n",
    "\n",
    "Função de perda (ou de custo): Mede o quão ruim está a previsão da rede em relação à resposta esperada. Calcula o erro da rede. É calculado no forward e usado no backpropagation para ajustar os pesos. Utilizando: Binary cross entropy (medida antes da função de ativação da saída)\n",
    "\n",
    "Patience: A partir de quantas iterações sem melhoria do erro eu devo parar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camadas = [2, 3, 1]\n",
    "fx_ativacao = 'relu'\n",
    "probabilidade_dropout = 0.0 # sem dropout\n",
    "\n",
    "modelo = MLP(camadas, fx_ativacao, probabilidade_dropout)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
